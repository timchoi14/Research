{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Exit Class\n",
    "class EarlyExit(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(EarlyExit, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))  # Global average pooling\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet with Early Exits\n",
    "class ResNet34WithExits(nn.Module):\n",
    "    def __init__(self, num_classes=10, exit_thresholds=(0.55,0.8)):\n",
    "        super(ResNet34WithExits, self).__init__()\n",
    "        # Define layers\n",
    "        self.num_classes = num_classes\n",
    "        self.exit_thresholds = exit_thresholds # confidence thresholds\n",
    "\n",
    "        # Load the ResNet34 backbone\n",
    "        resnet = models.resnet34(weights=None)\n",
    "        self.base = nn.Sequential(*list(resnet.children())[:-2])  # Exclude FC and Adaptive Pooling layers\n",
    "\n",
    "        # Early exit 1\n",
    "        self.exit1 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        # Early exit 2\n",
    "        self.exit2 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        # Final layers (ResNet output)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x, train_mode=True):\n",
    "        # Early exit 1\n",
    "        x1 = self.base[:3](x)\n",
    "        exit1_out = self.exit1(x1)\n",
    "\n",
    "        # Early exit 2\n",
    "        x2 = self.base[3:5](x1)\n",
    "        exit2_out = self.exit2(x2)\n",
    "\n",
    "        # Final exit\n",
    "        x3 = self.base[5:](x2)\n",
    "        x3 = self.avgpool(x3)\n",
    "        x3 = torch.flatten(x3, 1)\n",
    "        final_out = self.fc(x3)\n",
    "\n",
    "        # If in training mode, return all exits\n",
    "        if train_mode:\n",
    "            return exit1_out, exit2_out, final_out\n",
    "\n",
    "        # Otherwise, use threshold logic for inference\n",
    "        confidence1 = F.softmax(exit1_out, dim=1).max(1)[0]\n",
    "        if confidence1.mean().item() >= self.exit_thresholds[0]:\n",
    "            return exit1_out, None, None\n",
    "\n",
    "        confidence2 = F.softmax(exit2_out, dim=1).max(1)[0]\n",
    "        if confidence2.mean().item() >= self.exit_thresholds[1]:\n",
    "            return None, exit2_out, None\n",
    "\n",
    "        return None, None, final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = ResNet34WithExits(num_classes=10)  # Change `num_classes` as per your dataset\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for CIFAR-100\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define model, loss function, optimizer, and scheduler\n",
    "model = ResNet34WithExits(num_classes=10)  # ResNet34WithExits defined earlier\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_model(model, train_loader, optimizer, criterion, device, epochs):\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with early exits\n",
    "            exit1_out, exit2_out, final_out = model(inputs)\n",
    "\n",
    "            # Compute loss (using only the final output for simplicity)\n",
    "            loss = criterion(final_out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"resnet34_early_exit_cifar10_loweestT.pth\")\n",
    "    print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_all_exits(model, train_loader, optimizer, criterion, device, exit_weights, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a multi-exit model using a unified loss function.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The model with early exits.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        optimizer: Optimizer for the model parameters.\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        device: The device to run the training on ('cpu' or 'cuda').\n",
    "        exit_weights: List of weights for the losses of each exit [w1, w2, w3].\n",
    "        epochs: Number of training epochs.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = [0, 0, 0]  # Track correct predictions for each exit\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (train_mode=True ensures all exits produce outputs)\n",
    "            exit1_out, exit2_out, final_out = model(inputs, train_mode=True)\n",
    "\n",
    "            # Compute losses for each exit\n",
    "            loss1 = criterion(exit1_out, labels)\n",
    "            loss2 = criterion(exit2_out, labels)\n",
    "            loss3 = criterion(final_out, labels)\n",
    "\n",
    "            # Combine losses using exit weights\n",
    "            total_loss = (exit_weights[0] * loss1 +\n",
    "                          exit_weights[1] * loss2 +\n",
    "                          exit_weights[2] * loss3)\n",
    "\n",
    "            # Backward pass and optimizer step\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update statistics\n",
    "            running_loss += total_loss.item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Compute accuracy for each exit\n",
    "            for i, output in enumerate([exit1_out, exit2_out, final_out]):\n",
    "                _, predictions = torch.max(output, 1)\n",
    "                correct[i] += (predictions == labels).sum().item()\n",
    "\n",
    "        # Print epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = [100 * c / total if total > 0 else 0 for c in correct]\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Accuracies: Exit 1: {epoch_acc[0]:.2f}%, Exit 2: {epoch_acc[1]:.2f}%, Final Exit: {epoch_acc[2]:.2f}%\")\n",
    "    torch.save(model.state_dict(), \"resnet34_early_exit_cifar10_loweestT.pth\")\n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 1.8460, Accuracies: Exit 1: 24.01%, Exit 2: 33.71%, Final Exit: 35.52%\n",
      "Epoch [2/10] - Loss: 1.5280, Accuracies: Exit 1: 30.77%, Exit 2: 48.69%, Final Exit: 50.29%\n",
      "Epoch [3/10] - Loss: 1.3710, Accuracies: Exit 1: 32.65%, Exit 2: 57.21%, Final Exit: 57.98%\n",
      "Epoch [4/10] - Loss: 1.2630, Accuracies: Exit 1: 34.11%, Exit 2: 62.82%, Final Exit: 63.10%\n",
      "Epoch [5/10] - Loss: 1.1959, Accuracies: Exit 1: 35.07%, Exit 2: 66.27%, Final Exit: 66.36%\n",
      "Epoch [6/10] - Loss: 1.1446, Accuracies: Exit 1: 36.12%, Exit 2: 68.95%, Final Exit: 68.96%\n",
      "Epoch [7/10] - Loss: 1.1117, Accuracies: Exit 1: 36.42%, Exit 2: 70.53%, Final Exit: 70.14%\n",
      "Epoch [8/10] - Loss: 1.0866, Accuracies: Exit 1: 36.99%, Exit 2: 71.77%, Final Exit: 71.46%\n",
      "Epoch [9/10] - Loss: 1.0667, Accuracies: Exit 1: 37.18%, Exit 2: 72.66%, Final Exit: 72.24%\n",
      "Epoch [10/10] - Loss: 1.0456, Accuracies: Exit 1: 37.77%, Exit 2: 73.49%, Final Exit: 73.09%\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define weights for each exit's loss\n",
    "exit_weights = [0.3, 0.5, 0.2]  # Example: Emphasize Exit 1 more\n",
    "\n",
    "# Train the model\n",
    "train_model_with_all_exits(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    exit_weights=exit_weights,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_exit_tracking(model, test_loader, device, exit_thresholds=None):\n",
    "    \"\"\"\n",
    "    Test the multi-exit model, calculate overall accuracy, and track exit usage.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The trained model with early exits.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        device: The device to perform testing ('cpu' or 'cuda').\n",
    "        exit_thresholds: List of thresholds for early exits (optional; defaults to model thresholds).\n",
    "        \n",
    "    Returns:\n",
    "        overall_accuracy: The overall accuracy of the model across all exits.\n",
    "        exit_usage: A list with the count of samples exiting at each exit.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    exit_counts = [0, 0, 0]  # Track how many samples exit at each point\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Optionally override the model's thresholds\n",
    "    if exit_thresholds is not None:\n",
    "        model.exit_thresholds = exit_thresholds\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass with early exit logic\n",
    "            exit1_out, exit2_out, final_out = model(inputs, train_mode=False)\n",
    "\n",
    "            # Determine which exit is used for each batch\n",
    "            for i in range(inputs.size(0)):  # Iterate over batch size\n",
    "                outputs = None\n",
    "                if exit1_out is not None and exit1_out[i] is not None:\n",
    "                    outputs = exit1_out[i]\n",
    "                    exit_counts[0] += 1\n",
    "                elif exit2_out is not None and exit2_out[i] is not None:\n",
    "                    outputs = exit2_out[i]\n",
    "                    exit_counts[1] += 1\n",
    "                elif final_out is not None and final_out[i] is not None:\n",
    "                    outputs = final_out[i]\n",
    "                    exit_counts[2] += 1\n",
    "\n",
    "                # Compute predictions\n",
    "                if outputs is not None:\n",
    "                    _, prediction = torch.max(outputs.unsqueeze(0), 1)\n",
    "                    correct_predictions += (prediction == labels[i]).item()\n",
    "                    total_samples += 1\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = 100 * correct_predictions / total_samples if total_samples > 0 else 0\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "    for i, count in enumerate(exit_counts, start=1):\n",
    "        print(f\"Exit {i} Usage: {count} samples ({100 * count / total_samples:.2f}%)\")\n",
    "\n",
    "    return overall_accuracy, exit_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timot\\AppData\\Local\\Temp\\ipykernel_1404\\2985785703.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"resnet34_early_exit_cifar10_loweestT.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 69.83%\n",
      "Exit 1 Usage: 0 samples (0.00%)\n",
      "Exit 2 Usage: 10000 samples (100.00%)\n",
      "Exit 3 Usage: 0 samples (0.00%)\n"
     ]
    }
   ],
   "source": [
    "#load the model \n",
    "model = ResNet34WithExits(num_classes=10, exit_thresholds=(0.55, 0.6))\n",
    "model.load_state_dict(torch.load(\"resnet34_early_exit_cifar10_loweestT.pth\"))\n",
    "model = model.to(device)\n",
    "# Test the model\n",
    "accuracy, exit_usage = test_model_with_exit_tracking(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    exit_thresholds=[0.55, 0.7]  # Optional custom thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is where teh dapative starts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet with Early Exits(start of implementation)\n",
    "class ResNet34WithExits(nn.Module):\n",
    "    def __init__(self, num_classes=10, exit_thresholds=(0.55,0.8)):\n",
    "        super(ResNet34WithExits, self).__init__()\n",
    "        # Define layers\n",
    "        self.num_classes = num_classes\n",
    "        self.exit_thresholds = exit_thresholds # confidence thresholds\n",
    "\n",
    "        # Load the ResNet34 backbone\n",
    "        resnet = models.resnet34(weights=None)\n",
    "        self.base = nn.Sequential(*list(resnet.children())[:-2])  # Exclude FC and Adaptive Pooling layers\n",
    "\n",
    "        # Early exit 1\n",
    "        self.exit1 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        # Early exit 2\n",
    "        self.exit2 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        # Final layers (ResNet output)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x, train_mode=True):\n",
    "        # Early exit 1\n",
    "        x1 = self.base[:3](x)\n",
    "        exit1_out = self.exit1(x1)\n",
    "\n",
    "        # Early exit 2\n",
    "        x2 = self.base[3:5](x1)\n",
    "        exit2_out = self.exit2(x2)\n",
    "\n",
    "        # Final exit\n",
    "        x3 = self.base[5:](x2)\n",
    "        x3 = self.avgpool(x3)\n",
    "        x3 = torch.flatten(x3, 1)\n",
    "        final_out = self.fc(x3)\n",
    "\n",
    "        # If in training mode, return all exits\n",
    "        if train_mode:\n",
    "            return exit1_out, exit2_out, final_out\n",
    "\n",
    "        # Otherwise, use threshold logic for inference\n",
    "        confidence1 = F.softmax(exit1_out, dim=1).max(1)[0]\n",
    "        if confidence1.mean().item() >= self.exit_thresholds[0]:\n",
    "            return exit1_out, None, None\n",
    "\n",
    "        confidence2 = F.softmax(exit2_out, dim=1).max(1)[0]\n",
    "        if confidence2.mean().item() >= self.exit_thresholds[1]:\n",
    "            return None, exit2_out, None\n",
    "\n",
    "        return None, None, final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.filters import sobel\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "def compute_texture_variance(image):\n",
    "    \"\"\"Compute the variance of pixel intensities in the image.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return np.var(gray)\n",
    "\n",
    "def compute_edge_density(image):\n",
    "    \"\"\"Compute the density of edges in the image using Sobel filter.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    edges = sobel(gray)  # Sobel edge detection\n",
    "    return np.mean(edges > 0.1)  # Ratio of edge pixels\n",
    "\n",
    "def compute_color_variance(image):\n",
    "    \"\"\"Compute the variance of the color channels.\"\"\"\n",
    "    return np.var(image, axis=(0, 1)).mean()  # Variance across all channels\n",
    "\n",
    "def compute_entropy(image):\n",
    "    \"\"\"Compute the entropy of the image.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return shannon_entropy(gray)\n",
    "\n",
    "def adjust_thresholds(image, base_threshold):\n",
    "    texture_var = compute_texture_variance(image)\n",
    "    edge_density = compute_edge_density(image)\n",
    "    color_var = compute_color_variance(image)\n",
    "    entropy = compute_entropy(image)\n",
    "\n",
    "    adjusted_thresholds = [\n",
    "        base_threshold[0] + 0.01 * texture_var,\n",
    "        base_threshold[1] + 0.02 * edge_density - 0.1 * color_var,\n",
    "        base_threshold[2] + 0.01 * entropy\n",
    "    ]\n",
    "\n",
    "    adjusted_thresholds = [min(max(t,0),1) for t in adjusted_thresholds]\n",
    "    return adjusted_thresholds\n",
    "\n",
    "#model = ResNet34WithExits(num_classes=10, exit_thresholds=(e1, e2))  # Change `num_classes` as per your dataset\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for CIFAR-100\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define model, loss function, optimizer, and scheduler\n",
    "model = ResNet34WithExits(num_classes=10)  # ResNet34WithExits defined earlier\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_dynamic_thresholds(\n",
    "    model, train_loader, optimizer, criterion, device, base_thresholds, exit_weights, epochs\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model with dynamic thresholds based on image properties.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The model with early exits.\n",
    "        train_loader: DataLoader for the training dataset.\n",
    "        optimizer: Optimizer for training.\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        device: Device to train on ('cpu' or 'cuda').\n",
    "        base_thresholds: Base thresholds for dynamic adjustment.\n",
    "        exit_weights: Weights for losses of each exit.\n",
    "        epochs: Number of training epochs.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialize total loss for this batch\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Process each sample in the batch to calculate thresholds\n",
    "            for i in range(inputs.size(0)):\n",
    "                # Convert the image to numpy (H, W, C) for property calculations\n",
    "                image = inputs[i].permute(1, 2, 0).cpu().numpy()\n",
    "                adjusted_thresholds = adjust_thresholds(image, base_thresholds)\n",
    "\n",
    "                # Update the model's thresholds\n",
    "                model.exit_thresholds = adjusted_thresholds\n",
    "\n",
    "                # Forward pass for the sample\n",
    "                exit1_out, exit2_out, final_out = model(inputs[i].unsqueeze(0), train_mode=True)\n",
    "\n",
    "                '''\n",
    "                # Compute losses for all exits\n",
    "                if exit1_out is not None:\n",
    "                    loss1 = criterion(exit1_out, labels[i].unsqueeze(0))\n",
    "                    total_loss += exit_weights[0] * loss1\n",
    "                if exit2_out is not None:\n",
    "                    loss2 = criterion(exit2_out, labels[i].unsqueeze(0))\n",
    "                    total_loss += exit_weights[1] * loss2\n",
    "                if final_out is not None:\n",
    "                    loss3 = criterion(final_out, labels[i].unsqueeze(0))\n",
    "                    total_loss += exit_weights[2] * loss3\n",
    "                '''\n",
    "                # Compute losses for each exit\n",
    "                loss1 = criterion(exit1_out, labels)\n",
    "                loss2 = criterion(exit2_out, labels)\n",
    "                loss3 = criterion(final_out, labels)\n",
    "\n",
    "                # Combine losses using exit weights\n",
    "                total_loss = (exit_weights[0] * loss1 +\n",
    "                            exit_weights[1] * loss2 +\n",
    "                            exit_weights[2] * loss3)\n",
    "                \n",
    "                # Backpropagation and optimization\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update running loss\n",
    "                running_loss += total_loss.item()\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / total_samples:.4f}\"\n",
    "        )\n",
    "    torch.save(model.state_dict(), \"resnet34_early_exit_cifar10_loweestT.pth\")\n",
    "    print(\"Model saved successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_model_dynamic_thresholds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_thresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_thresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexit_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[115], line 44\u001b[0m, in \u001b[0;36mtrain_model_dynamic_thresholds\u001b[1;34m(model, train_loader, optimizer, criterion, device, base_thresholds, exit_weights, epochs)\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mexit_thresholds \u001b[38;5;241m=\u001b[39m adjusted_thresholds\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Forward pass for the sample\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m exit1_out, exit2_out, final_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m# Compute losses for all exits\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mif exit1_out is not None:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    total_loss += exit_weights[2] * loss3\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Compute losses for each exit\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[112], line 45\u001b[0m, in \u001b[0;36mResNet34WithExits.forward\u001b[1;34m(self, x, train_mode)\u001b[0m\n\u001b[0;32m     42\u001b[0m exit2_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit2(x2)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Final exit\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x3)\n\u001b[0;32m     47\u001b[0m x3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x3, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\resnet.py:93\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     92\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m---> 93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2810\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2798\u001b[0m         batch_norm,\n\u001b[0;32m   2799\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2807\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2808\u001b[0m     )\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m-> 2810\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2813\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2814\u001b[0m     weight,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2821\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled,\n\u001b[0;32m   2822\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\timot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2776\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2774\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   2775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 2776\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2777\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2778\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "base_thresholds = [0.5, 0.6, 0.7]  # Base thresholds for dynamic adjustment\n",
    "exit_weights = [0.4, 0.3, 0.3]  # Weights for exit losses\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 10\n",
    "\n",
    "# Train the model\n",
    "train_model_dynamic_thresholds(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    base_thresholds=base_thresholds,\n",
    "    exit_weights=exit_weights,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
